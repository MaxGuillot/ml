{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Le-Machine-Learning\" data-toc-modified-id=\"Le-Machine-Learning-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Le Machine Learning</a></span></li><li><span><a href=\"#Des-Réseaux-de-Neurones\" data-toc-modified-id=\"Des-Réseaux-de-Neurones-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Des Réseaux de Neurones</a></span></li><li><span><a href=\"#Le-Perceptron-Multicouche\" data-toc-modified-id=\"Le-Perceptron-Multicouche-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Le Perceptron Multicouche</a></span></li></ul></li><li><span><a href=\"#Paramètrage\" data-toc-modified-id=\"Paramètrage-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Paramètrage</a></span><ul class=\"toc-item\"><li><span><a href=\"#Fonction-d'activation\" data-toc-modified-id=\"Fonction-d'activation-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Fonction d'activation</a></span></li><li><span><a href=\"#Fonction-de-coût\" data-toc-modified-id=\"Fonction-de-coût-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Fonction de coût</a></span></li><li><span><a href=\"#Taux-d'apprentissage\" data-toc-modified-id=\"Taux-d'apprentissage-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Taux d'apprentissage</a></span></li><li><span><a href=\"#Seuil-de-convergence\" data-toc-modified-id=\"Seuil-de-convergence-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Seuil de convergence</a></span></li></ul></li><li><span><a href=\"#Structure-du-réseau\" data-toc-modified-id=\"Structure-du-réseau-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Structure du réseau</a></span><ul class=\"toc-item\"><li><span><a href=\"#Les-couches\" data-toc-modified-id=\"Les-couches-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Les couches</a></span></li><li><span><a href=\"#Les-neurones\" data-toc-modified-id=\"Les-neurones-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Les neurones</a></span></li></ul></li><li><span><a href=\"#Implémentation\" data-toc-modified-id=\"Implémentation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Implémentation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Partis-pris\" data-toc-modified-id=\"Partis-pris-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Partis pris</a></span></li><li><span><a href=\"#Neurone\" data-toc-modified-id=\"Neurone-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Neurone</a></span></li><li><span><a href=\"#Propagation-avant\" data-toc-modified-id=\"Propagation-avant-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Propagation avant</a></span></li><li><span><a href=\"#Propagation-arrière\" data-toc-modified-id=\"Propagation-arrière-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Propagation arrière</a></span></li><li><span><a href=\"#Couche\" data-toc-modified-id=\"Couche-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Couche</a></span></li><li><span><a href=\"#Réseau\" data-toc-modified-id=\"Réseau-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>Réseau</a></span></li></ul></li><li><span><a href=\"#Limitations\" data-toc-modified-id=\"Limitations-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Limitations</a></span></li><li><span><a href=\"#Améliorations-futures\" data-toc-modified-id=\"Améliorations-futures-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Améliorations futures</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction\n",
    "===\n",
    "\n",
    "Le Machine Learning\n",
    "---\n",
    "\n",
    "Le réseau de neurones est une des composantes d'un champ particulier de l'Intelligence Artificielle, le Machine Learning (ML), traduit par Apprentissage Automatique en français. Le ML permet à un agent logiciel d'apprendre de façon systèmatique à produire un certain résultat à partir de données labellisées.\n",
    "\n",
    "Les applications de ce domaine sont nombreuses et croissantes, comme par exemple la partition d'ensembles à des fins commerciales : quels sont les acheteurs compulsifs, les réguliers ? Une fois le modèle établi de ML établi, il devient possible d'estimer le comportement pour d'autres cas (prédiction).\n",
    "\n",
    "![Le Machine Learning par Dilbert](http://www.nuageo.fr/wp-content/uploads/2016/12/machine-learning-robots-dilbert.gif)\n",
    "\n",
    "Des Réseaux de Neurones\n",
    "---\n",
    "\n",
    "Un réseau de neurones se base sur la modèle très simplifié d'un cerveau. Ce cerveau est une succession de couches de neurones liées entre elles. Le neurone est un automate qui reçoit de la couche précédente un influx qui l'activera ou non. Cette activation est modélisée par une fonction d'ativation, présente dans tous les neurones du réseau. \n",
    "\n",
    "La façon dont ces couches sont agencées et le nombre de neurones à l'intérieur vont permettre différents types d'apprentissages :\n",
    "* perception\n",
    "* regroupement\n",
    "\n",
    "Dans le cas présent, il s'agit de perception (d'où Perceptron), à savoir percevoir l'issue à partir d'un cas d'entrée. \n",
    "\n",
    "> Matrice X des exemples avec Y la matrice des résultats connus.\n",
    "\n",
    "![Le cas des données XKCD()](https://imgs.xkcd.com/comics/machine_learning.png)\n",
    "\n",
    "Le Perceptron Multicouche\n",
    "---\n",
    "En anglais Multi Layered Perceptron (MLP), ces réseaux de neurones permettent de modéliser et apprendre des systèmes complexes, avec une couche d'entrée, une couche de sortie, et entre les deux, un ensemble de couches cachées.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paramètrage\n",
    "===\n",
    "\n",
    "A l'instar de toutes les méthodes de ML, il convient de préciser un certain nombre de paramètres. \n",
    "\n",
    "Fonction d'activation\n",
    "---\n",
    "\n",
    "Pour des raisons de simplicité, la fonction ReLu qui renvoie le maximum entre un réel et 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Fonction d'activation simple\n",
    "# x scalaire\n",
    "def frelu(x):\n",
    "\tif x>0:\n",
    "\t\treturn x\n",
    "\telse:\n",
    "\t\treturn 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testons pour x = -3 et x = 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cas pour x = -3 : 0| Cas pour x = 3 : 3\n"
     ]
    }
   ],
   "source": [
    "print(\"Cas pour x = -3 : \"+str(frelu(-3))+\"| Cas pour x = 3 : \" + str(frelu(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction de coût\n",
    "---\n",
    "\n",
    "La fonction de coût est ce qui permet de mesurer l'erreur entre ce qu'estime le réseau de neurones et la valeur actuelle. Le but est d'améliorer les poids des neurones jusqu'à ce que l'erreur soit acceptable.\n",
    "\n",
    "Pour le cas présent, la foncton sera l'erreur moyenne au carré."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fonction de cout, moyenne des moindres carres\n",
    "#e vecteur des estimateurs\n",
    "#y vecteur de resultat\n",
    "def fcout_mmc (e,y):\n",
    "\tif len(e) == len(y):\n",
    "\t\tm = len(y)\n",
    "\t\tres = 0\n",
    "\t\tfor i in range(0,m):\n",
    "\t\t\tres += (e[i] - y[i])*(e[i] - y[i])\n",
    "\t\tres = res / m\n",
    "\t\treturn res\n",
    "\telse:\n",
    "\t\tprint(\"Longueurs e et y distinctes !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testons-la avec des vecteurs [1,0] et [0.8,0.1], où l'erreur vaut 0,25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02499999999999999"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [1,0]\n",
    "e = [0.8,0.1]\n",
    "fcout_mmc(e,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taux d'apprentissage\n",
    "---\n",
    "\n",
    "Représenté par la lettre alpha, il représente la vitesse à laquelle la correction est appliquée dans le réseau. Il est inférieur à 1.\n",
    "\n",
    "Un taux d'apprentissage grand risque de faire diverger la fonction du coût. Au contraire, un taux très petit va considérablement augmenter le nombre d'itérations nécessaires pour faire converger cette fonction de coût sur un minimum, voire, le faire rester sur un minimum local.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seuil de convergence\n",
    "---\n",
    "Représenté par la lettre eta, il représente le seul à partir duquel l'erreur est considérée acceptable, entre 0 et 1 en général.\n",
    "\n",
    "Un seuil trop petit augmente les chances de surapprentissage, un seuil trop grand diminuera la précision du modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure du réseau\n",
    "===\n",
    "\n",
    "![MLP](img/rnn.png)\n",
    "\n",
    "Les couches\n",
    "---\n",
    "\n",
    "Un réseau se compose de :\n",
    "* une couche d'entrée _ec_ qui comporte autant de neurones que de variables dans les vecteurs d'entrée ;\n",
    "* une couche cachée _cc_ qui comporte autant de neurones que voulu, souvent 3 ou 4 ;\n",
    "* une couche de sortie _sc_ qui comporte autant de neurones que de variables dans les vecteur de sortie.\n",
    "\n",
    "Les neurones\n",
    "---\n",
    "\n",
    "Un neurone se modélise par :\n",
    "* une valeur ;\n",
    "* un tableau de poids, un poids pour chaque lien sortant ;\n",
    "* une erreur.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implémentation\n",
    "===\n",
    "\n",
    "Partis pris\n",
    "---\n",
    "Afin de faciliter l'évolution du code, une approche objet sera observée. Il y aura trois objets :\n",
    "* Neurone qui sera la brique de base dans laquelle se feront la majeure partie des calculs ;\n",
    "* Couche qui sera un tableau de neurones, avec quelques fonctions d'agrégations\n",
    "* Reseau qui sera l'assemblage de _ec_, _cc_ et _sc_, avec les fcontions d'initations de calculs à l'échelle du réseau.\n",
    "\n",
    "Neurone\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Classe neurone\n",
    "class Neurone:\n",
    "\n",
    "# t_sor taille vecteur sor, len([0])\n",
    "\tdef __init__(self, t_sor,alpha):\n",
    "\t\t#err ne saurait servir dans avec une couche cachee\n",
    "\t\tself.err = 0\n",
    "\t\tself.val = 0\n",
    "\t\tself.alpha = alpha\n",
    "\t\tself.t_sor = t_sor\n",
    "\t\tself.grad = 0\n",
    "\t\tself.poids = np.random.random((t_sor))\t\t\n",
    "\n",
    "# Accesseurs m_x pour modifier, l_x pour retourner\n",
    "\tdef m_err(self,x):\n",
    "\t\tself.err = x\n",
    "\n",
    "\tdef m_val(self,x):\n",
    "\t\tself.val = x\n",
    "\n",
    "\t# avec i le ieme poids du neurone\n",
    "\tdef m_grad(self,x):\n",
    "\t\tself.grad = x\n",
    "\n",
    "\tdef m_poids(self,i,x):\n",
    "\t\tself.poids[i] = x\n",
    "\n",
    "\tdef l_poids(self,i):\n",
    "\t\treturn self.poids[i]\n",
    "\n",
    "\tdef l_grad(self):\n",
    "\t\treturn self.grad\n",
    "\n",
    "\tdef l_val(self):\n",
    "\t\treturn self.val\t\n",
    "\n",
    "\tdef l_err(self):\n",
    "\t\treturn self.err\n",
    "\n",
    "\tdef l_tpoids(self):\n",
    "\t\tres = []\n",
    "\t\tfor p in self.poids:\n",
    "\t\t\tres.append(p)\n",
    "\t\treturn res\n",
    "\n",
    "\t# Prop avant\n",
    "\t# x valeurs d'entree\n",
    "\t# w valeurs des poids\n",
    "\tdef naprop(self, x, w):\n",
    "\t\tres = 0\t\t\n",
    "\t\tif len(x) != len(w):\n",
    "\t\t\tprint(\"Erreur dim poids et vec entrant !\")\n",
    "\t\telse:\n",
    "\t\t\tres = np.dot(x,w)\n",
    "\t\t\tres = frelu(res)\n",
    "\t\t\tself.m_val(res)\n",
    "\n",
    "\t# Prop arriere\n",
    "\t# tdy tableau d'erreurs de la couche d'apres\n",
    "\tdef nrprop(self, tdy):\n",
    "\t\tif len(tdy) != len(self.poids):\n",
    "\t\t\tprint(\"Poids et erreurs de tailles distinctes !\")\n",
    "\t\t\tprint(self.l_tpoids())\n",
    "\t\t\tprint(tdy)\n",
    "\t\telse:\n",
    "\t\t\tgrad = self.l_val() * np.dot(self.poids,tdy)\n",
    "\t\t\tself.m_grad(grad)\n",
    "\t\t\tcpt = 0\n",
    "\t\t\tfor p in self.poids:\n",
    "\t\t\t\tpoids = p - self.alpha * grad\n",
    "\t\t\t\tself.m_poids(cpt,poids)\n",
    "\t\t\t\tcpt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Propagation avant\n",
    "---\n",
    "\n",
    "Pour chaque neurone (j) de la couche (k), la valeur est égale au produit scalaire des valeurs de la couche (k-1) noté __x__ par les poids de la couche (k-1) vers le neurone (j) notés __w__.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction d'activation est appliquée après le produit scalaire. L'utilisation des accesseurs permet à coup sûr de répercuter les modifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Propagation arrière\n",
    "---\n",
    "\n",
    "Pour chaque neurone (j) de la couche (k), le gradient vaut la valeur du neurone (j) noté __val__ multiplié par le produit scalaire des erreurs de la couche (k+1) noté __tdy__ par les poids du neurone (k) notés __poids__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Couche\n",
    "---\n",
    "\n",
    "La couche permet de lancer les fonctions de propagation avant et arrière pour ses neurones. Elle permet également d'accéder aux différentes valeurs de ses neurones, sous la forme de tableaux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Couche:\n",
    "\t# nn Nombre de neurones dans la couche\n",
    "\t# t_sor dimension du vecteur de sortie\n",
    "\tdef __init__(self, nn, t_sor, alpha):\n",
    "\t\tself.neu = []\t\t\n",
    "\t\tfor i in range(nn):\n",
    "\t\t\tself.neu.append(Neurone(t_sor,alpha))\n",
    "\t\t# Ponderation poids par dimension de la couche\n",
    "\t\t# He et al, 2015\t\n",
    "\t\t# Utile en haute dimension\n",
    "\t\tpond = np.sqrt(2) / np.sqrt(np.maximum(nn,2))\n",
    "\t\tfor n in self.neu:\n",
    "\t\t\tfor i in range(t_sor):\n",
    "\t\t\t\tpoids = n.l_poids(i) * pond\n",
    "\t\t\t\tn.m_poids(i,poids)\n",
    "\t\t\n",
    "\n",
    "\t# Retourne les valeurs des neurones de la couche\n",
    "\tdef clval(self):\n",
    "\t\tres = []\n",
    "\t\tfor n in self.neu:\n",
    "\t\t\tres.append(n.l_val())\n",
    "\t\treturn res\n",
    "\n",
    "\t# Retourne les poids des neurones de la couche\n",
    "\tdef clpoids(self):\n",
    "\t\tres= []\n",
    "\t\tfor n in self.neu:\n",
    "\t\t\tres.append(n.l_tpoids())\n",
    "\t\treturn res\n",
    "\n",
    "\t#Retourn les gradients des neurones de la couche\n",
    "\tdef clgrad(self):\n",
    "\t\tres = []\n",
    "\t\tfor n in self.neu:\n",
    "\t\t\tres.append(n.l_grad())\n",
    "\t\treturn res\n",
    "\n",
    "\t# Prop avant\n",
    "\t# x valeurs d'entree\n",
    "\t# W tableau des valeurs des poids\n",
    "\tdef caprop(self,x,W):\n",
    "\t\tcpt = 0\t\t\n",
    "\t\tfor n in self.neu:\n",
    "\t\t\tw = []\n",
    "\t\t\tfor tp in W:\n",
    "\t\t\t\tw.append(tp[cpt])\n",
    "\t\t\tn.naprop(x,w)\n",
    "\t\t\tcpt +=1\n",
    "\n",
    "\t# Prop arr\n",
    "\t# tdy tableau d'erreurs de la couche d'apres\n",
    "\tdef crprop(self,tdy):\n",
    "\t\tfor n in self.neu:\n",
    "\t\t\tn.nrprop(tdy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Réseau\n",
    "---\n",
    "Le réseau agit en chef d'orchestre pour les différents calculs : il lance les calculs sur les couches et transmet le taux d'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Reseau:\n",
    "\t# nc de neurones dans couche cachee\n",
    "\t# t_ent = len(X[0])\n",
    "\t# t_sor = len(y[0])\n",
    "\t# eta taux de convergence\n",
    "\tdef __init__(self,nc,t_ent,t_sor,eta,alpha):\n",
    "\t\tself.eta = eta\n",
    "\t\t#Couche entree\n",
    "\t\tself.ec = Couche(t_ent,nc,alpha)\n",
    "\t\t# Couche cachee\n",
    "\t\tself.cc = Couche(nc,t_sor,alpha)\n",
    "\t\t# Couche de sortie\n",
    "\t\tself.sc = Couche(t_sor, 0,alpha)\n",
    "\t\t# Tableau des couts\n",
    "\t\tself.tc = []\n",
    "\t\t\n",
    "\t# Accesseurs\n",
    "\tdef m_tc(self,x):\n",
    "\t\tself.tc.append(x)\n",
    "\n",
    "\t# Lance les prop a et r\n",
    "\tdef calc(self,X,Y):\n",
    "\t\tif len(X)!=len(Y):\n",
    "\t\t\tprint(\"Donnees entrantes de dimensions distinctes !\")\n",
    "\t\telse:\n",
    "\t\t\ta = len(X)\n",
    "\t\t\ttemp_c = []\n",
    "\t\t\tfor i in range(a):\n",
    "\t\t\t\tx = X[i]\n",
    "\t\t\t\ty = Y[i]\n",
    "\n",
    "\t\t\t\t# Init couche entrante\n",
    "\t\t\t\tcpt=0\n",
    "\t\t\t\tfor n in self.ec.neu:\n",
    "\t\t\t\t\tn.m_val(x[cpt])\n",
    "\t\t\t\t\tcpt+=1\n",
    "\t\t\t\tcpt=0\n",
    "\n",
    "\t\t\t\t# Prop avant cc\n",
    "\t\t\t\tself.cc.caprop(self.ec.clval(),self.ec.clpoids())\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Prop avant sc\n",
    "\t\t\t\tself.sc.caprop(self.cc.clval(),self.cc.clpoids())\n",
    "\t\t\t\te = self.sc.clval()\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Init couche sortie retro\n",
    "\t\t\t\ttdy = np.subtract(e,y)\n",
    "\t\t\t\tself.cc.crprop(tdy)\n",
    "\t\t\t\t#print(\"Example :\" + str(i))\n",
    "\t\t\t\ttemp_c.append(fcout_mmc(self.sc.clval(),y))\n",
    "\t\t\tself.m_tc(np.mean(temp_c))\n",
    "\n",
    "\t# Fait propager le modele\t\n",
    "\t# X vecteurs entree\n",
    "\t# Y vecteurs sortie\n",
    "\tdef sconv(self,X,Y,epoch):\n",
    "\t\tinter = False\n",
    "\t\tfor ep in range(epoch):\n",
    "\t\t\tself.calc(X,Y)\n",
    "\t\t\t#print(self.tc)\n",
    "\t\t\tif ep >= 100 and self.tc[-1] <= self.eta:\n",
    "\t\t\t\tprint(\"Convergence atteinte a l'epoch \" + str(ep))\n",
    "\t\t\t\tprint(self.tc[ep])\n",
    "\t\t\t\tinter = True\n",
    "\t\t\t\tbreak\n",
    "\t\t\telif ep >= 100 and ((self.tc[-2] - self.tc[-1])/self.tc[-2]) < 0.000001:\n",
    "\t\t\t\tprint(\"Plus d'amelioration a partir de l'epoch \" + str(ep))\n",
    "\t\t\t\tprint(self.tc[ep])\n",
    "\t\t\t\tinter = True\n",
    "\t\t\t\tbreak\n",
    "\t\tif inter == False:\n",
    "\t\t\tprint(\"Fin iteration \" + str(epoch) + \" avec un cout de \" + str(self.tc[-1]))\n",
    "\n",
    "\t# Effectue le test a partir des donnees \n",
    "\t# x vecteur d'entree\n",
    "\tdef test(self,x):\n",
    "\t\t# Init couche entrante\n",
    "\t\tcpt=0\n",
    "\t\tfor n in self.ec.neu:\n",
    "\t\t\tn.m_val(x[cpt])\n",
    "\t\t\tcpt+=1\t\t\t\t\n",
    "\t\tcpt=0\n",
    "\n",
    "\t\t# Prop avant cc\n",
    "\t\tself.cc.caprop(self.ec.clval(),self.ec.clpoids())\n",
    "\t\t\n",
    "\t\t# Prop avant sc\t\t\n",
    "\t\tself.sc.caprop(self.cc.clval(),self.cc.clpoids())\n",
    "\t\te = self.sc.clval()\n",
    "\t\treturn e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A la ligne 28, il faut noter la subtilité suivante : le tableau des poids d'un neurone (i) d'une couche (k) n'est pas l'ensemble des poids de la couche (k) qui pointent sur le neurone (j) d'une couche (k+1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=[[1,0,1,0],[1,0,1,1],[0,1,0,1]]\n",
    "Y = [[1],[1],[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin iteration 5000 avec un cout de 0.0103401547014\n",
      "Le test !\n",
      "[0.81673500701007395]\n"
     ]
    }
   ],
   "source": [
    "monres=Reseau(2*len(X[0]),len(X[0]),len(Y[0]),0.01,0.01)\n",
    "monres.sconv(X,Y,5000)\n",
    "print(\"Le test !\")\n",
    "print(monres.test([0.9,0.2,0.8,0.1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limitations\n",
    "===\n",
    "\n",
    "* Pas de validation croisée, impossibilité de  limiter le sur-apprentissage\n",
    "* Convergence longue, précision limitée\n",
    "* N'accepte que des vecteurs python basiques, pas de np.ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Améliorations futures\n",
    "===\n",
    "* Permettre plusieurs couches cachées\n",
    "* Gérer des np.ndarray\n",
    "* ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
